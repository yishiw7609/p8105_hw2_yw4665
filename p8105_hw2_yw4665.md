p8105_hw2_yw4665
================
Yishi Wang
2025-09-23

## Package calls

Call necessary libraries here:

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.2
    ## ✔ ggplot2   4.0.0     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

## Problem 1

Read desired data from the local_files folder and assign them to
according variables:

``` r
pols_month_df = read_csv("local_files/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names()
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemployment_df = read_csv("local_files/fivethirtyeight_datasets/unemployment.csv") |>
  janitor::clean_names()
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp_df = read_csv("local_files/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names()
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Clean the the pols_month dataframe first and assign the result df to
‘cleaned_pols_month_df’:

``` r
cleaned_pols_month_df = pols_month_df |> 
  separate("mon", c("year", "month", "day"), "-") |> 
  mutate(
    month = month.name[as.integer(month)],
    president = if_else(prez_gop == 0, "dem", "gop"),
    year = as.numeric(year)
    ) |>
  select(-prez_gop, -prez_dem, -day)
```

Now clean the snp dataframe with similar procedures:

``` r
cleaned_snp_df = snp_df |>
  separate("date", c("month", "day", "year"), "/") |>
  mutate(
    month = month.name[as.integer(month)],
    # Change the year formatting to match with pols_month_df
    year = if_else(as.integer(year) < 30, as.integer(year) + 2000, as.integer(year) + 1900 ),
  ) |> 
  select(year, month, close)
```

Now tidy the unemployment dataframe to match the previous dataframes:

``` r
cleaned_unemployment_df = unemployment_df |>
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment_rate"
    ) |>
  mutate(
    month = month.name[match(month, tolower(month.abb))]
    )
```

Now we combine all three dataframes through two left_joins:

``` r
pols_snp_unempolyment_df = cleaned_pols_month_df |>
  left_join(cleaned_snp_df, by = c("year", "month")) |>
  left_join(cleaned_unemployment_df, by = c("year", "month"))
```

Three datasets each contains information about an event over a period of
time, specifically speaking:

- pos-month: includes number of governors from republican and democrat
  parties and the party of presidency from 1947-01 to 2015-06
- snp: includes Standard & Poor’s stock market index from 1950-01 to
  2015-07
- unempolyment: includes unemployment rate from 1948-01 to 2015-12

By combining the datasets above with date (year-month), we are able to
see the impact of politician parties on SnP stock market and
unempolyment rate. These data over time may reflect the overall
performance of a party or presidency.

## Problem02

First, import the Mr. Trash Wheel sheet and apply changes:

``` r
mtw_df = read_excel("local_files/202509 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N709") |>
  janitor::clean_names() |>
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    category = "Mr. Trash Wheel",
    year = as.numeric(year),
  )  |>
  filter(!is.na(dumpster))
```

Similarly, for Professor Trash Wheel and Gwynnda datasets:

``` r
ptw_df = read_excel("local_files/202509 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M134") |>
  janitor::clean_names() |>
  mutate(
    category = "Professor Trash Wheel"
  ) |>
  filter(!is.na(dumpster))

gftw_df = read_excel("local_files/202509 Trash Wheel Collection Data.xlsx", sheet = "Gwynns Falls Trash Wheel", range = "A2:L351") |>
  janitor::clean_names() |>
  mutate(
    category = "Gwynns Falls Trash Wheel"
  ) |>
  filter(!is.na(dumpster))
```

Now we combine all 3 datasets by stacking them together:

``` r
all_trash_wheels_df = bind_rows(mtw_df, ptw_df, gftw_df)
```

Lastly, the combined Trash Wheels df contains 1188 observations across
three wheels (Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda). Key
variables include `year`, `month` and `date` (collection date),
`weight_tons` and `volume_cubic_yards` (mass and volume of collected
trash). `plastic_bottles:sport_balls` (all types of recycled trash)
`homes_powered` (estimated number of household to be powered) and
`category` (which Trash Wheel contributed).

``` r
all_trash_wheels_df |>
  filter(category == "Professor Trash Wheel") |>
  summarise(total = sum(weight_tons, na.rm = TRUE)) |>
  pull(total)
```

    ## [1] 282.26

We see that Professor Trash Wheel collected a total of 282.26 tons of
trash.

``` r
all_trash_wheels_df |> 
  filter(category == "Gwynns Falls Trash Wheel", year == 2022, month == "June") |> 
  summarise(total = sum(cigarette_butts, na.rm = TRUE)) |> 
  pull(total)
```

    ## [1] 18120

And Gwynnda collected 18120 cigarette butts during June 2022.

## Problem 3

First import two datasets:

``` r
zip_codes_df = read_csv("local_files/zillow_data/Zip Codes.csv") |>
  janitor::clean_names()
```

    ## Rows: 322 Columns: 7
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (4): County, County Code, File Date, Neighborhood
    ## dbl (3): State FIPS, County FIPS, ZipCode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
zip_zori_df = read_csv("local_files/zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |>
  janitor::clean_names() 
```

    ## Rows: 149 Columns: 125
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr   (6): RegionType, StateName, State, City, Metro, CountyName
    ## dbl (119): RegionID, SizeRank, RegionName, 2015-01-31, 2015-02-28, 2015-03-3...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Perform tidying for zip_codes_df:

``` r
zip_codes_clean = zip_codes_df |>
  mutate(
    county = str_to_title(county),
    neighborhood = if_else(is.na(neighborhood), NA_character_, str_to_title(neighborhood))
  ) |>
  distinct() |>
  select(zip_code, county, county_fips, county_code, state_fips, file_date, neighborhood, everything())
```

Perform tidying for zip_zori_df:

``` r
zori_clean = zip_zori_df |>
  pivot_longer(
    cols = contains("x20"),
    names_to = "date",
    values_to = "zori_value",
    values_transform = list(zori_value = as.numeric)
  ) |>
  mutate(
    city = if_else(is.na(city) | city == "", NA_character_, str_to_title(city))
  ) |>
  filter(!is.na(zori_value)) |>
  select(region_id, size_rank, region_name, region_type, state, city, county_name, date, zori_value, metro) 
```

Merging two datasets with zip_codes:

``` r
zori_with_zipinfo = zori_clean |>
  full_join(zip_codes_clean, by = c("region_name" = "zip_code")) |>
  select(date, zori_value, everything())
```

    ## Warning in full_join(zori_clean, zip_codes_clean, by = c(region_name = "zip_code")): Detected an unexpected many-to-many relationship between `x` and `y`.
    ## ℹ Row 2759 of `x` matches multiple rows in `y`.
    ## ℹ Row 256 of `y` matches multiple rows in `x`.
    ## ℹ If a many-to-many relationship is expected, set `relationship =
    ##   "many-to-many"` to silence this warning.

``` r
summary(zori_with_zipinfo)
```

    ##      date             zori_value     region_id        size_rank    
    ##  Length:10848       Min.   :1102   Min.   : 61615   Min.   :    4  
    ##  Class :character   1st Qu.:2243   1st Qu.: 61642   1st Qu.:  172  
    ##  Mode  :character   Median :2702   Median : 61807   Median :  710  
    ##                     Mean   :2889   Mean   : 69187   Mean   : 2263  
    ##                     3rd Qu.:3373   3rd Qu.: 62034   3rd Qu.: 2551  
    ##                     Max.   :8423   Max.   :399546   Max.   :30490  
    ##                     NA's   :171    NA's   :171      NA's   :171    
    ##   region_name    region_type           state               city          
    ##  Min.   :10001   Length:10848       Length:10848       Length:10848      
    ##  1st Qu.:10028   Class :character   Class :character   Class :character  
    ##  Median :10463   Mode  :character   Mode  :character   Mode  :character  
    ##  Mean   :10641                                                           
    ##  3rd Qu.:11221                                                           
    ##  Max.   :11697                                                           
    ##                                                                          
    ##  county_name           metro              county           county_fips   
    ##  Length:10848       Length:10848       Length:10848       Min.   :36005  
    ##  Class :character   Class :character   Class :character   1st Qu.:36047  
    ##  Mode  :character   Mode  :character   Mode  :character   Median :36061  
    ##                                                           Mean   :36057  
    ##                                                           3rd Qu.:36061  
    ##                                                           Max.   :36085  
    ##                                                                          
    ##  county_code          state_fips  file_date         neighborhood      
    ##  Length:10848       Min.   :36   Length:10848       Length:10848      
    ##  Class :character   1st Qu.:36   Class :character   Class :character  
    ##  Mode  :character   Median :36   Mode  :character   Mode  :character  
    ##                     Mean   :36                                        
    ##                     3rd Qu.:36                                        
    ##                     Max.   :36                                        
    ## 

``` r
length(unique(zori_with_zipinfo$region_name))
```

    ## [1] 320

``` r
length(unique(zori_with_zipinfo$neighborhood))
```

    ## [1] 43

We can see that after pivoting longer, there is a total of 320 unique
zip codes and 43 neighborhoods in the combined dataset, and the total
number of observation is 10848. Now to find the Zip Codes that appear in
one dataset and not in the other:

``` r
zips_zip_codes = zip_codes_clean$zip_code |>
  unique() |>
  sort()

zips_zori = zori_clean$region_name |>
  unique() |>
  sort()

zips_in_zori_only = setdiff(zips_zori, zips_zip_codes)
zips_in_zori_only
```

    ## numeric(0)

``` r
length(zips_in_zori_only)
```

    ## [1] 0

``` r
zips_in_zip_codes_only = setdiff(zips_zip_codes, zips_zori)
zips_in_zip_codes_only
```

    ##   [1] 10008 10020 10041 10043 10045 10047 10048 10055 10072 10080 10081 10082
    ##  [13] 10087 10101 10102 10103 10104 10105 10106 10107 10108 10109 10110 10111
    ##  [25] 10112 10113 10114 10115 10116 10117 10118 10119 10120 10121 10122 10123
    ##  [37] 10124 10125 10126 10129 10130 10131 10132 10133 10138 10149 10150 10151
    ##  [49] 10152 10153 10154 10155 10156 10157 10158 10159 10160 10161 10163 10164
    ##  [61] 10165 10166 10167 10168 10169 10170 10171 10172 10173 10174 10175 10176
    ##  [73] 10177 10178 10179 10185 10197 10199 10213 10242 10249 10256 10259 10260
    ##  [85] 10261 10265 10268 10269 10270 10271 10272 10273 10274 10275 10276 10277
    ##  [97] 10278 10279 10281 10285 10286 10292 10302 10307 10309 10310 10311 10313
    ## [109] 10464 10474 10475 10499 10550 10704 10705 10803 11001 11004 11005 11040
    ## [121] 11096 11202 11224 11239 11241 11242 11243 11245 11247 11251 11252 11256
    ## [133] 11351 11352 11359 11362 11363 11371 11380 11381 11386 11405 11411 11412
    ## [145] 11413 11414 11416 11417 11419 11420 11421 11422 11423 11424 11425 11427
    ## [157] 11428 11429 11430 11431 11433 11436 11439 11451 11499 11559 11580 11690
    ## [169] 11694 11695 11697

``` r
length(zips_in_zip_codes_only)
```

    ## [1] 171

We can see thtat 171 unique zip codes appear in zip_codes datasets but
not in the Zori dataset. After looking a few zip codes up, I find that
these are PO box / corporate mail ZIP, and these PO-box-only ZIPs have
no housing rental observations, so Zori is not computed.

Lastly, to find the impact of COVID-19 by constructing a table:
